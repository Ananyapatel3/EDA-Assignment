{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e86914-3bc5-4403-81bb-8b28b1fac075",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435297e4-c6bb-40cb-aab5-253ddc1c94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Mathematical Formula for a Linear SVM\n",
    "A linear SVM aims to find the optimal hyperplane that separates two classes. The equation of the hyperplane is:\n",
    "[ w^T x + b = 0 ]\n",
    "where:\n",
    "- ( w ) is the weight vector,\n",
    "- ( x ) is the input feature vector,\n",
    "- ( b ) is the bias term.\n",
    "Q2: Objective Function of a Linear SVM\n",
    "The objective function of a linear SVM is:\n",
    "[ \\min \\frac{1}{2} ||w||{n} \\xi_i ]\n",
    "where:\n",
    "- ( w^2 ) ensures maximum margin,\n",
    "- ( C ) is the regularization parameter,\n",
    "- ( \\xi_i ) are slack variables allowing misclassification.\n",
    "Q3: Kernel Trick in SVM\n",
    "The kernel trick allows SVM to handle non-linearly separable data by mapping it into a higher-dimensional space without explicitly computing the transformation. Common kernels include:\n",
    "- Linear Kernel: ( K(x_i, x_j) = x_i^T x_j )\n",
    "- Polynomial Kernel: ( K(x_i, x_j) = (x_i^T x_j + c)^d )\n",
    "- RBF Kernel: ( K(x_i, x_j) = \\exp(-\\gamma x_i - x_j^2) ).\n",
    "Q4: Role of Support Vectors in SVM (Example)\n",
    "Support vectors are the closest data points to the hyperplane and define its position. They maximize the margin between classes.\n",
    "Example: In a binary classification problem, support vectors are the points that lie closest to the decision boundary.\n",
    "Q5: Illustrations of Hyperplane, Marginal Plane, Soft Margin, and Hard Margin\n",
    "- Hyperplane: The decision boundary separating classes.\n",
    "- Marginal Plane: The boundary defining the margin.\n",
    "- Hard Margin: No misclassification allowed.\n",
    "- Soft Margin: Allows some misclassification for better generalization.\n",
    "Q6: SVM Implementation on the Iris Dataset\n",
    "Here’s how you can implement an SVM classifier using scikit-learn:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "clf = SVC(kernel=\"linear\", C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot decision boundaries\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
    "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\")\n",
    "plt.title(\"SVM Decision Boundary on Iris Dataset\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Bonus Task: Implementing a Linear SVM from Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d5bd3-7485-4547-88c1-92ee1fb8f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2e56e-6e9a-48d5-a0b7-8c941ab70653",
   "metadata": {},
   "outputs": [],
   "source": [
    "Great set of questions! Let's break them down:\n",
    "Q1: Relationship Between Polynomial Functions and Kernel Functions in Machine Learning\n",
    "Polynomial functions are used in kernel functions to transform data into higher-dimensional spaces, allowing Support Vector Machines (SVMs) to classify non-linearly separable data. The polynomial kernel is defined as:\n",
    "[ K(x, y) = (x^T y + c)^d ]\n",
    "where:\n",
    "- ( x ) and ( y ) are feature vectors,\n",
    "- ( c ) is a constant,\n",
    "- ( d ) is the polynomial degree.\n",
    "This transformation enables SVMs to find a non-linear decision boundary.\n",
    "Q2: Implementing an SVM with a Polynomial Kernel in Python Using Scikit-learn\n",
    "Here’s how you can implement an SVM with a polynomial kernel:\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, :2]  # Using two features for visualization\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM with polynomial kernel\n",
    "svm_poly = SVC(kernel=\"poly\", degree=3, C=1.0)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = svm_poly.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Polynomial Kernel SVM Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "This implementation uses a degree-3 polynomial kernel to classify the Iris dataset.\n",
    "Q3: Effect of Increasing Epsilon on Support Vectors in SVR\n",
    "In Support Vector Regression (SVR), epsilon (ε) defines a margin within which predictions are not penalized. Increasing ε:\n",
    "- Reduces the number of support vectors, as fewer points fall outside the margin.\n",
    "- Increases tolerance for errors, leading to a simpler model.\n",
    "- Can decrease accuracy, as fewer points influence the regression function.\n",
    "Q4: Effect of Kernel, C, Epsilon, and Gamma on SVR Performance\n",
    "Each parameter affects SVR differently:\n",
    "- Kernel Function: Determines how data is transformed.\n",
    "- Linear: Best for simple relationships.\n",
    "- Polynomial/RBF: Handles complex patterns.\n",
    "- C Parameter: Controls regularization.\n",
    "- High C: Less regularization, more complex model.\n",
    "- Low C: More regularization, simpler model.\n",
    "- Epsilon (ε): Defines tolerance for errors.\n",
    "- High ε: Fewer support vectors, simpler model.\n",
    "- Low ε: More support vectors, complex model.\n",
    "- Gamma (γ): Controls influence of data points.\n",
    "- High γ: Focuses on nearby points, risk of overfitting.\n",
    "- Low γ: Considers distant points, smoother decision boundary.\n",
    "Q5: Assignment - SVM Implementation\n",
    "Here’s a structured approach:\n",
    "- Import Libraries & Load Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "- Split Dataset\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "- Preprocess Data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "- Train SVM Classifier\n",
    "svm_model = SVC(kernel=\"linear\", C=1.0)\n",
    "svm_model.fit(X_train, y_train)\n",
    "- Predict & Evaluate\n",
    "y_pred = svm_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "- Hyperparameter Tuning\n",
    "param_grid = {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"poly\", \"rbf\"]}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "- Train Tuned Model\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "- Save Model for Future Use\n",
    "with open(\"svm_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c66ac-0f2a-4025-bc4c-aa75b0492f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
