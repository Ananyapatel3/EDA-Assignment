{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f83b2f-66fb-432a-addd-31591e21228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive bayes assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294588fd-a875-4d09-a21b-4726ce2a020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What is Bayes' Theorem?\n",
    "Bayes' Theorem is a fundamental concept in probability theory that describes how to update the probability of a hypothesis based on new evidence. It is widely used in statistics, machine learning, and decision-making.\n",
    "Q2: Formula for Bayes' Theorem\n",
    "The formula for Bayes' Theorem is:\n",
    "[ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} ]\n",
    "where:\n",
    "- ( P(AB) ) is the probability of event A given event B,\n",
    "- ( P(BA) ) is the probability of event B given event A,\n",
    "- ( P(A) ) is the prior probability of event A,\n",
    "- ( P(B) ) is the probability of event B.\n",
    "Q3: How is Bayes' Theorem Used in Practice?\n",
    "Bayes' Theorem is used in various real-world applications, including:\n",
    "- Spam Filtering: Email systems classify messages as spam or not spam based on word frequency.\n",
    "- Medical Diagnosis: Doctors update the probability of a disease given test results.\n",
    "- Weather Forecasting: Meteorologists refine predictions based on new data.\n",
    "- Financial Forecasting: Investors assess risks based on market trends.\n",
    "Q4: Relationship Between Bayes' Theorem and Conditional Probability\n",
    "Bayes' Theorem is an extension of conditional probability, which calculates the likelihood of an event occurring given another event has already happened. It allows us to reverse conditional probabilities, making it useful for inference and decision-making.\n",
    "Q5: Choosing the Right Naive Bayes Classifier\n",
    "There are three main types of Naive Bayes classifiers:\n",
    "- Gaussian Naive Bayes â€“ Used for continuous data assuming a normal distribution.\n",
    "- Multinomial Naive Bayes â€“ Best for text classification and word frequency data.\n",
    "- Bernoulli Naive Bayes â€“ Suitable for binary feature data (e.g., presence or absence of words in spam filtering).\n",
    "The choice depends on the nature of the dataset.\n",
    "Q6: Assignment - Classifying a New Instance Using Naive Bayes\n",
    "To classify ( X1 = 3 ) and ( X2 = 4 ), we calculate:\n",
    "[ P(A) = P(B) = 0.5 \\quad \\text{(equal priors)} ]\n",
    "Using likelihood estimation:\n",
    "[ P(X1=3 | A) = \\frac{4}{10}, \\quad P(X2=4 | A) = \\frac{3}{10} ]\n",
    "[ P(X1=3 | B) = \\frac{1}{7}, \\quad P(X2=4 | B) = \\frac{3}{7} ]\n",
    "Computing posterior probabilities:\n",
    "[ P(A | X1=3, X2=4) = \\frac{(4/10) \\times (3/10) \\times 0.5}{P(X1=3, X2=4)} ]\n",
    "[ P(B | X1=3, X2=4) = \\frac{(1/7) \\times (3/7) \\times 0.5}{P(X1=3, X2=4)} ]\n",
    "Since ( P(A  X1=3, X2=4) ) is higher, Naive Bayes predicts Class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559fcb01-c478-49b4-bfea-17def80df2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive bayes assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb6f38-c0f9-402d-bf5d-4c4fc388e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Probability of an Employee Being a Smoker Given They Use the Health Insurance Plan\n",
    "Using Bayes' Theorem, we calculate:\n",
    "[ P(\\text{Smoker} | \\text{Health Insurance}) = \\frac{P(\\text{Health Insurance} | \\text{Smoker}) \\times P(\\text{Smoker})}{P(\\text{Health Insurance})} ]\n",
    "Given:\n",
    "- ( P(\\text{Health Insurance}) = 0.70 )\n",
    "- ( P(\\text{Smoker}  \\text{Health Insurance}) = 0.40 )\n",
    "Thus:\n",
    "[ P(\\text{Smoker} | \\text{Health Insurance}) = \\frac{0.40 \\times 0.70}{0.70} = 0.40 ]\n",
    "So, the probability that an employee is a smoker given they use the health insurance plan is 40%.\n",
    "Q2: Difference Between Bernoulli Naive Bayes and Multinomial Naive Bayes\n",
    "- Bernoulli Naive Bayes: Used for binary features (presence/absence of words).\n",
    "- Multinomial Naive Bayes: Used for count-based features (word frequency in text classification).\n",
    "- Key Difference: Bernoulli models whether a feature exists, while Multinomial models how often a feature appears.\n",
    "Q3: Handling Missing Values in Bernoulli Naive Bayes\n",
    "Bernoulli Naive Bayes handles missing values by:\n",
    "- Ignoring missing attributes during probability calculations.\n",
    "- Using Laplace smoothing to prevent zero probabilities.\n",
    "- Dropping missing records if necessary.\n",
    "Q4: Can Gaussian Naive Bayes Be Used for Multi-Class Classification?\n",
    "Yes! Gaussian Naive Bayes can handle multi-class classification by modeling each class with a Gaussian distribution. It works well when features follow a normal distribution.\n",
    "Q5: Assignment - Implementing Naive Bayes on the Spambase Dataset\n",
    "Steps:\n",
    "- Download the dataset from UCI Machine Learning Repository.\n",
    "- Preprocess the data (handle missing values, normalize features).\n",
    "- Split into training/testing sets.\n",
    "- Train Bernoulli, Multinomial, and Gaussian Naive Bayes classifiers using scikit-learn.\n",
    "- Evaluate performance using:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-score\n",
    "- Perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV.\n",
    "- Train the best model on the full dataset.\n",
    "- Save the trained model using pickle.\n",
    "- Upload the Jupyter Notebook to GitHub and share the repository link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315bc05-b382-4520-af96-8d2f79f4b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble technique Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de78a3-a860-4851-bf49-76198340063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Great set of questions! Let's break them down:\n",
    "Q1: What is an Ensemble Technique in Machine Learning?\n",
    "Ensemble techniques combine multiple machine learning models to improve accuracy and robustness. Instead of relying on a single model, ensemble methods aggregate predictions from multiple models to reduce errors and enhance generalization.\n",
    "Q2: Why Are Ensemble Techniques Used in Machine Learning?\n",
    "Ensemble techniques are used to:\n",
    "- Improve accuracy by leveraging multiple models.\n",
    "- Reduce variance and prevent overfitting.\n",
    "- Enhance stability by combining diverse models.\n",
    "- Handle complex patterns that individual models may miss.\n",
    "Q3: What is Bagging?\n",
    "Bagging (Bootstrap Aggregating) is an ensemble method that trains multiple models independently on different random subsets of the training data. Their predictions are then combinedâ€”usually by averaging (for regression) or voting (for classification)â€”to improve accuracy and reduce variance.\n",
    "Q4: What is Boosting?\n",
    "Boosting is an ensemble technique where models are trained sequentially, with each new model focusing on correcting the errors made by previous models. The final prediction is a weighted combination of all models, reducing bias and improving accuracy.\n",
    "Q5: Benefits of Using Ensemble Techniques\n",
    "- Higher accuracy compared to individual models.\n",
    "- Better generalization to unseen data.\n",
    "- Reduced overfitting by averaging multiple models.\n",
    "- Improved robustness against noisy data.\n",
    "Q6: Are Ensemble Techniques Always Better Than Individual Models?\n",
    "Not always! While ensembles often improve performance, they:\n",
    "- Increase computational cost due to multiple models.\n",
    "- May not be necessary if a single model performs well.\n",
    "- Can be harder to interpret compared to simpler models.\n",
    "Q7: How Is the Confidence Interval Calculated Using Bootstrap?\n",
    "Bootstrap estimates confidence intervals by:\n",
    "- Resampling the dataset with replacement.\n",
    "- Computing the statistic (e.g., mean) for each sample.\n",
    "- Generating a distribution of the statistic.\n",
    "- Finding the percentiles (e.g., 2.5% and 97.5%) to form the confidence interval.\n",
    "Q8: How Does Bootstrap Work? Steps Involved\n",
    "- Draw multiple random samples (with replacement) from the dataset.\n",
    "- Compute the statistic (e.g., mean) for each sample.\n",
    "- Repeat the process thousands of times.\n",
    "- Analyze the distribution of the computed statistics.\n",
    "- Determine confidence intervals using percentiles.\n",
    "Q9: Estimating the 95% Confidence Interval for Tree Heights Using Bootstrap\n",
    "Given:\n",
    "- Sample mean = 15 meters\n",
    "- Standard deviation = 2 meters\n",
    "- Sample size = 50 trees\n",
    "Using the bootstrap method:\n",
    "- Resample the dataset (with replacement) multiple times.\n",
    "- Compute the mean for each resampled dataset.\n",
    "- Sort the means and find the 2.5th and 97.5th percentiles.\n",
    "- Estimate the confidence interval.\n",
    "Using a normal approximation:\n",
    "[ CI = \\text{Mean} \\pm 1.96 \\times \\frac{\\text{Standard Deviation}}{\\sqrt{\\text{Sample Size}}} ]\n",
    "[ CI = 15 \\pm 1.96 \\times \\frac{2}{\\sqrt{50}} ]\n",
    "[ CI = 15 \\pm 0.55 ]\n",
    "Thus, the 95% confidence interval is (14.45, 15.55) meters.\n",
    "Would you like me to help with code implementation for any of these tasks? ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c70bba-4818-4b60-9fd3-53237020d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble technique Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1a137-d321-447d-83b8-a97dea81accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Great set of questions! Let's break them down:\n",
    "Q1: How Does Bagging Reduce Overfitting in Decision Trees?\n",
    "Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by training multiple models on different random subsets of the data and averaging their predictions. Since decision trees tend to overfit by capturing noise in the training data, bagging helps by:\n",
    "- Reducing varianceâ€”each tree sees a different subset of data, preventing over-reliance on specific patterns.\n",
    "- Improving generalizationâ€”averaging multiple trees smooths out extreme predictions.\n",
    "- Preventing memorizationâ€”individual trees may overfit, but the ensemble mitigates this.\n",
    "Q2: Advantages and Disadvantages of Different Base Learners in Bagging\n",
    "Advantages:\n",
    "- Decision Trees: Handle non-linear relationships well, work with categorical and numerical data.\n",
    "- Support Vector Machines (SVMs): Effective for high-dimensional data.\n",
    "- Neural Networks: Capture complex patterns but require more computational power.\n",
    "Disadvantages:\n",
    "- Decision Trees: Prone to overfitting without bagging.\n",
    "- SVMs: Computationally expensive for large datasets.\n",
    "- Neural Networks: Require extensive tuning and training.\n",
    "Q3: How Does the Choice of Base Learner Affect the Bias-Variance Tradeoff in Bagging?\n",
    "- High-bias models (e.g., linear regression): Bagging has limited impact since bias remains.\n",
    "- High-variance models (e.g., decision trees): Bagging reduces variance, improving generalization.\n",
    "- Complex models (e.g., neural networks): Bagging can help, but boosting may be more effective.\n",
    "Q4: Can Bagging Be Used for Both Classification and Regression? How Does It Differ?\n",
    "Yes! Bagging works for both:\n",
    "- Classification: Uses majority voting among models.\n",
    "- Regression: Uses averaging of predictions. The key difference is how predictions are aggregatedâ€”classification relies on discrete votes, while regression averages continuous values.\n",
    "Q5: Role of Ensemble Size in Bagging & How Many Models to Include\n",
    "- Larger ensembles improve stability but increase computation.\n",
    "- Typical range: 10â€“100 models, depending on dataset size.\n",
    "- Trade-off: More models reduce variance but may not significantly improve accuracy beyond a certain point.\n",
    "Q6: Real-World Application of Bagging in Machine Learning\n",
    "Bagging is widely used in:\n",
    "- Random Forests: A bagging-based ensemble of decision trees for classification and regression.\n",
    "- Fraud Detection: Helps reduce false positives by aggregating multiple models.\n",
    "- Medical Diagnosis: Improves accuracy in predicting diseases by combining multiple classifiers.\n",
    "Would you like me to help with code implementation for any of these concepts? ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131961d9-06fe-4733-b353-8a7cbec616eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble technique Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf838f8c-0778-41f3-a8de-fe2c9d822268",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What is Random Forest Regressor?\n",
    "Random Forest Regressor is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and reduce overfitting. It is widely used for regression tasks, where the goal is to predict continuous values.\n",
    "Q2: How Does Random Forest Regressor Reduce Overfitting?\n",
    "Random Forest reduces overfitting by:\n",
    "- Bagging (Bootstrap Aggregating): Training each tree on a different subset of the data.\n",
    "- Feature Randomization: Selecting a random subset of features for each tree, preventing reliance on specific variables.\n",
    "- Averaging Predictions: Combining multiple trees' outputs smooths extreme predictions and reduces variance.\n",
    "Q3: How Does Random Forest Regressor Aggregate Predictions?\n",
    "Each decision tree in the forest makes an independent prediction. The final output is obtained by averaging the predictions of all trees, ensuring a more stable and accurate result.\n",
    "Q4: What Are the Hyperparameters of Random Forest Regressor?\n",
    "Key hyperparameters include:\n",
    "- n_estimators: Number of trees in the forest.\n",
    "- max_depth: Maximum depth of each tree.\n",
    "- min_samples_split: Minimum samples required to split a node.\n",
    "- min_samples_leaf: Minimum samples required at a leaf node.\n",
    "- max_features: Number of features considered for each split.\n",
    "- bootstrap: Whether to sample data with replacement.\n",
    "Q5: Difference Between Random Forest Regressor and Decision Tree Regressor\n",
    "| Feature | Random Forest Regressor | Decision Tree Regressor | \n",
    "| Nature | Ensemble of multiple trees | Single decision tree | \n",
    "| Overfitting | Less prone due to averaging | More prone, especially deep trees | \n",
    "| Interpretability | Harder to interpret | Easy to visualize | \n",
    "| Computational Cost | Higher | Lower | \n",
    "| Performance on Large Data | Better generalization | May struggle with complex data | \n",
    "\n",
    "\n",
    "Q6: Advantages and Disadvantages of Random Forest Regressor\n",
    "Advantages:\n",
    "- High accuracy due to ensemble averaging.\n",
    "- Handles missing values effectively.\n",
    "- Works well with large datasets.\n",
    "- Reduces overfitting compared to single decision trees.\n",
    "Disadvantages:\n",
    "- Computationally expensive due to multiple trees.\n",
    "- Less interpretable than a single decision tree.\n",
    "- Slower prediction time compared to simpler models.\n",
    "Q7: What Is the Output of Random Forest Regressor?\n",
    "The output is a continuous numerical value, obtained by averaging predictions from multiple decision trees.\n",
    "Q8: Can Random Forest Regressor Be Used for Classification Tasks?\n",
    "No, Random Forest Regressor is specifically designed for regression tasks. However, Random Forest Classifier is used for classification problems, where predictions are made using majority voting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289fb6d-4d10-4cb0-8fc7-ef536a563ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble technique Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d2d15-ac1f-4a45-8467-e6450ab7effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Building a Machine Learning Pipeline for Feature Engineering and Handling Missing Values\n",
    "Below is a structured approach to building the pipeline:\n",
    "Step 1: Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "Step 2: Load the Dataset\n",
    "# Load dataset (assuming it's a CSV file)\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df.drop(columns=[\"target\"])\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "num_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_features = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "\n",
    "Step 3: Feature Selection using Random Forest\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "feature_selector.fit(X, y)\n",
    "X_selected = feature_selector.transform(X)\n",
    "\n",
    "î·™î·š\n",
    "Explanation:\n",
    "- Uses Random Forest to select important features automatically.\n",
    "- Reduces dimensionality, improving model performance.\n",
    "Step 4: Numerical Pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  # Replace missing values with column mean\n",
    "    (\"scaler\", StandardScaler())  # Standardization for numerical features\n",
    "])\n",
    "\n",
    "î·™î·š\n",
    "Step 5: Categorical Pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Fill missing categorical values\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))  # Convert categorical values to numerical\n",
    "])\n",
    "\n",
    "î·™î·š\n",
    "Step 6: Combine Pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_features),\n",
    "    (\"cat\", cat_pipeline, cat_features)\n",
    "])\n",
    "\n",
    "\n",
    "Step 7: Final Model Pipeline with Random Forest Classifier\n",
    "final_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "î·›î·œî·™î·š\n",
    "Interpretation of Results\n",
    "- High accuracy suggests the pipeline effectively selects features and processes missing values.\n",
    "- If accuracy is low, consider:\n",
    "- Trying different imputation methods (median or KNN).\n",
    "- Tuning the hyperparameters of Random Forest.\n",
    "- Removing less relevant features.\n",
    "\n",
    "Q2: Voting Classifier with Random Forest and Logistic Regression on the Iris Dataset\n",
    "Step 1: Import Necessary Libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "Step 2: Load the Iris Dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "\n",
    "Step 3: Define Individual Models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "\n",
    "Step 4: Create Voting Classifier Pipeline\n",
    "voting_clf = VotingClassifier(estimators=[(\"rf\", rf), (\"lr\", lr)], voting=\"hard\")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"classifier\", voting_clf)\n",
    "])\n",
    "\n",
    "Step 5: Train and Evaluate Model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Voting Classifier Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "î·™î·š\n",
    "Interpretation of Results\n",
    "- Combining models with Voting Classifier improves overall accuracy.\n",
    "- Random Forest captures complex relationships, while Logistic Regression adds linear stability.\n",
    "- Further improvements:\n",
    "- Use soft voting if models provide probabilities.\n",
    "- Tune hyperparameters of individual classifiers.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
