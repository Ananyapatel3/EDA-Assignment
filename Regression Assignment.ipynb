{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e56d3-2906-41c1-a999-244fb7661538",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression Assignment 1\n",
    "Q1: Difference Between Simple and Multiple Linear Regression\n",
    "- Simple Linear Regression: Models the relationship between one independent variable and one dependent variable using a straight-line equation.\n",
    "Example: Predicting house price based on square footage.\n",
    "Equation: ( y = \\beta_0 + \\beta_1x )\n",
    "- Multiple Linear Regression: Extends simple regression by incorporating multiple independent variables.\n",
    "Example: Predicting house price based on square footage, number of bedrooms, and location.\n",
    "Equation: ( y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n )\n",
    "Q2: Assumptions of Linear Regression & How to Check Them\n",
    "- Linearity: Relationship between independent and dependent variables should be linear.\n",
    "- Check: Scatter plots or residual plots.\n",
    "- Independence: Observations should be independent.\n",
    "- Check: Durbin-Watson test for autocorrelation.\n",
    "- Homoscedasticity: Residuals should have constant variance.\n",
    "- Check: Residual vs. fitted value plots.\n",
    "- Normality: Residuals should be normally distributed.\n",
    "- Check: Q-Q plots or Shapiro-Wilk test.\n",
    "- No Multicollinearity: Independent variables should not be highly correlated.\n",
    "- Check: Variance Inflation Factor (VIF).\n",
    "Q3: Interpreting Slope and Intercept\n",
    "- Slope: Represents the change in the dependent variable for a one-unit increase in the independent variable.\n",
    "- Intercept: The predicted value of the dependent variable when all independent variables are zero.\n",
    "Example:\n",
    "If a salary prediction model is ( \\text{Salary} = 30,000 + 2,000 \\times \\text{Years of Experience} ),\n",
    "- The intercept (30,000) means a person with zero experience earns $30,000.\n",
    "- The slope (2,000) means each additional year of experience increases salary by $2,000.\n",
    "Q4: Concept of Gradient Descent in Machine Learning\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models.\n",
    "- Process:\n",
    "- Calculate the gradient (slope) of the cost function.\n",
    "- Update parameters by moving in the direction of the negative gradient.\n",
    "- Repeat until convergence.\n",
    "- Used in: Linear regression, logistic regression, neural networks.\n",
    "Q5: Multiple Linear Regression Model vs. Simple Linear Regression\n",
    "- Multiple Linear Regression: Uses multiple independent variables to predict the dependent variable.\n",
    "- Difference: More complex, accounts for multiple factors, requires checking for multicollinearity.\n",
    "Q6: Multicollinearity in Multiple Linear Regression\n",
    "- Definition: When independent variables are highly correlated, making coefficient estimates unreliable.\n",
    "- Detection:\n",
    "- Variance Inflation Factor (VIF) > 10 indicates multicollinearity.\n",
    "- Correlation matrix analysis.\n",
    "- Solution:\n",
    "- Remove highly correlated variables.\n",
    "- Use Principal Component Analysis (PCA).\n",
    "- Apply Ridge Regression.\n",
    "Q7: Polynomial Regression Model vs. Linear Regression\n",
    "- Polynomial Regression: Models non-linear relationships by introducing polynomial terms.\n",
    "Equation: ( y = \\beta_0 + \\beta_1x + \\beta_2x3 + ... )\n",
    "- Difference:\n",
    "- Linear regression fits a straight line.\n",
    "- Polynomial regression fits a curve.\n",
    "Q8: Advantages & Disadvantages of Polynomial Regression\n",
    "Advantages:\n",
    "- Captures complex relationships.\n",
    "- Provides better fit for non-linear data.\n",
    "Disadvantages:\n",
    "- Prone to overfitting.\n",
    "- Requires careful selection of polynomial degree.\n",
    "When to Use:\n",
    "- When data exhibits a clear non-linear trend.\n",
    "- When a simple linear model fails to capture patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35efa9-327b-489a-9f4e-2f87eab05169",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resgression Assignment 2\n",
    "Q1: Concept of R-squared in Linear Regression\n",
    "R-squared, also known as the coefficient of determination, measures how well a regression model explains the variance in the dependent variable. It is calculated as:\n",
    "[ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} ]\n",
    "where:\n",
    "- ( SS_{res} ) is the sum of squared residuals (errors),\n",
    "- ( SS_{tot} ) is the total sum of squares (variance of the dependent variable).\n",
    "A higher R-squared value (closer to 1) indicates a better fit, meaning the model explains more of the variance in the data.\n",
    "Q2: Adjusted R-squared vs. Regular R-squared\n",
    "Adjusted R-squared modifies R-squared to account for the number of predictors in the model. Unlike R-squared, which always increases when adding more variables (even if they are irrelevant), adjusted R-squared penalizes unnecessary predictors, ensuring only meaningful variables improve the model.\n",
    "Q3: When to Use Adjusted R-squared\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. It helps prevent overfitting by discouraging the inclusion of irrelevant variables.\n",
    "Q4: RMSE, MSE, and MAE in Regression Analysis\n",
    "- Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual values.\n",
    "- Mean Squared Error (MSE): Squares the errors before averaging, penalizing larger errors more.\n",
    "- Root Mean Squared Error (RMSE): Takes the square root of MSE, making it more interpretable in the same units as the dependent variable.\n",
    "Q5: Advantages & Disadvantages of RMSE, MSE, and MAE\n",
    "- MAE: Easy to interpret but treats all errors equally.\n",
    "- MSE: Penalizes large errors more but is harder to interpret due to squared units.\n",
    "- RMSE: Balances interpretability and sensitivity to large errors.\n",
    "Q6: Lasso vs. Ridge Regularization\n",
    "- Lasso (L1 Regularization): Shrinks coefficients and can set some to zero, effectively performing feature selection.\n",
    "- Ridge (L2 Regularization): Shrinks coefficients but does not eliminate features.\n",
    "- When to use: Lasso is better for sparse models, while Ridge is preferred when all features contribute meaningfully.\n",
    "Q7: Preventing Overfitting with Regularized Models\n",
    "Regularization prevents overfitting by penalizing large coefficients, ensuring the model generalizes well.\n",
    "Example: In predicting house prices, Ridge regression prevents extreme weight assignments to features like square footage, ensuring a balanced model.\n",
    "Q8: Limitations of Regularized Linear Models\n",
    "- May not work well with highly non-linear relationships.\n",
    "- Can overly shrink important features.\n",
    "- Requires careful tuning of regularization parameters.\n",
    "Q9: Comparing Model A (RMSE = 10) vs. Model B (MAE = 8)\n",
    "RMSE penalizes large errors more, while MAE treats all errors equally. If large errors are critical, Model A may be worse. However, choosing solely based on one metric has limitations—considering both is ideal.\n",
    "Q10: Comparing Ridge (0.1) vs. Lasso (0.5) Regularization\n",
    "- Ridge (0.1): Less aggressive regularization, retains all features.\n",
    "- Lasso (0.5): Stronger regularization, may eliminate features.\n",
    "- Trade-offs: Ridge is better for correlated features, while Lasso is useful for feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48c4c8-feaa-4716-b196-19e66d51787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression Assignment 3\n",
    "Great questions! Let's break them down:\n",
    "Q1: What is Ridge Regression, and how does it differ from Ordinary Least Squares (OLS)?\n",
    "Ridge Regression is a type of linear regression that includes L2 regularization, which helps prevent overfitting by penalizing large coefficients. Unlike OLS, which minimizes the sum of squared residuals, Ridge Regression minimizes:\n",
    "[ \\sum (y_i - \\hat{y}_i)2 ]\n",
    "where λ is the regularization parameter that controls the penalty on large coefficients. Ridge Regression is particularly useful when predictor variables are highly correlated, as it stabilizes coefficient estimates.\n",
    "Q2: Assumptions of Ridge Regression\n",
    "Ridge Regression shares most assumptions with OLS:\n",
    "- Linearity: The relationship between predictors and the target variable is linear.\n",
    "- Independence: Observations should be independent.\n",
    "- Homoscedasticity: Residuals should have constant variance.\n",
    "- No perfect multicollinearity: While Ridge Regression handles multicollinearity, extreme cases can still affect performance.\n",
    "Q3: Selecting the Tuning Parameter (Lambda)\n",
    "The value of λ is chosen using cross-validation:\n",
    "- Grid search: Testing multiple values and selecting the best-performing one.\n",
    "- Regularization path: Observing how coefficients shrink as λ increases.\n",
    "- Minimizing validation error: Choosing λ that minimizes prediction error.\n",
    "Q4: Can Ridge Regression be used for Feature Selection?\n",
    "While Ridge Regression shrinks coefficients, it does not set them to zero like Lasso Regression. However, it can reduce the impact of less important features, making it useful for feature selection when combined with other techniques.\n",
    "Q5: Performance in the Presence of Multicollinearity\n",
    "Ridge Regression handles multicollinearity well by shrinking correlated coefficients, preventing extreme fluctuations. This leads to more stable and generalizable models compared to OLS.\n",
    "Q6: Handling Categorical and Continuous Variables\n",
    "Yes! Ridge Regression can handle both categorical and continuous variables:\n",
    "- Continuous variables: Used directly.\n",
    "- Categorical variables: Must be encoded (e.g., one-hot encoding or label encoding) before applying Ridge Regression.\n",
    "Q7: Interpreting Ridge Regression Coefficients\n",
    "- Smaller coefficients: Ridge Regression shrinks coefficients, meaning their impact is reduced.\n",
    "- Relative importance: While coefficients are smaller, their relative ranking still indicates feature importance.\n",
    "- Bias-variance tradeoff: Ridge Regression introduces bias but reduces variance, leading to better generalization.\n",
    "Q8: Using Ridge Regression for Time-Series Analysis\n",
    "Yes, Ridge Regression can be applied to time-series data, especially when dealing with multicollinearity among lagged variables. However, it does not inherently model temporal dependencies, so it is often combined with autoregressive models or feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc34b15-d691-4804-8f24-ee5f7ad4f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression Assignment 4\n",
    "Q1: What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a regularization technique that adds an L1 penalty to the regression model. Unlike Ordinary Least Squares (OLS), which minimizes the sum of squared residuals, Lasso minimizes:\n",
    "[ \\sum (y_i - \\hat{y}_i)^2 + \\lambda \\sum |\\beta_j| ]\n",
    "where λ is the regularization parameter that controls the penalty on large coefficients. The key difference is that Lasso shrinks some coefficients to zero, effectively performing feature selection.\n",
    "Q2: Main Advantage of Lasso Regression in Feature Selection\n",
    "Lasso Regression automatically selects important features by shrinking irrelevant coefficients to zero. This makes it useful for high-dimensional datasets where feature selection is crucial.\n",
    "Q3: Interpreting Lasso Regression Coefficients\n",
    "- Non-zero coefficients: Represent features that contribute to the model.\n",
    "- Zero coefficients: Indicate features that have been eliminated.\n",
    "- Magnitude of coefficients: Shows the relative importance of each feature.\n",
    "Q4: Tuning Parameters in Lasso Regression\n",
    "The main tuning parameter is λ (lambda):\n",
    "- Small λ: Less regularization, more features retained.\n",
    "- Large λ: Stronger regularization, more coefficients shrink to zero.\n",
    "- Optimal λ: Found using cross-validation to balance bias and variance.\n",
    "Q5: Can Lasso Regression Be Used for Non-Linear Regression?\n",
    "Yes! Lasso can be applied to non-linear problems by transforming features (e.g., polynomial features) before applying Lasso. However, it does not inherently model non-linearity.\n",
    "Q6: Difference Between Ridge and Lasso Regression\n",
    "| Feature | Ridge Regression | Lasso Regression | \n",
    "| Regularization Type | L2 (squared coefficients) | L1 (absolute coefficients) | \n",
    "| Feature Selection | No (shrinks but retains all features) | Yes (sets some coefficients to zero) | \n",
    "| Handles Multicollinearity | Yes | Yes, but may arbitrarily select one correlated feature | \n",
    "\n",
    "\n",
    "Q7: Handling Multicollinearity in Lasso Regression\n",
    "Lasso reduces the impact of multicollinearity by selecting one feature from a group of correlated features while setting others to zero. However, Ridge Regression is often preferred when all correlated features are important.\n",
    "Q8: Choosing the Optimal Value of Lambda\n",
    "Lambda is selected using cross-validation:\n",
    "- Grid search: Testing multiple values.\n",
    "- Regularization path: Observing coefficient shrinkage.\n",
    "- Minimizing validation error: Choosing λ that balances bias and variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a0451-ff51-4f28-b1d9-6f73d1027616",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression Assignmen 5\n",
    "Q1: What is Elastic Net Regression and how does it differ from other regression techniques?\n",
    "Elastic Net Regression is a regularization technique that combines Lasso (L1 penalty) and Ridge (L2 penalty) regression. It balances feature selection (Lasso) and multicollinearity handling (Ridge), making it ideal for datasets with correlated features.\n",
    "Q2: How do you choose the optimal values of the regularization parameters for Elastic Net Regression?\n",
    "The two key parameters are:\n",
    "- Alpha (λ): Controls overall regularization strength.\n",
    "- L1 ratio (ρ): Determines the balance between Lasso and Ridge. Optimal values are chosen using cross-validation, typically through grid search or regularization path analysis.\n",
    "Q3: Advantages and Disadvantages of Elastic Net Regression\n",
    "Advantages:\n",
    "- Handles multicollinearity better than Lasso.\n",
    "- Performs feature selection while retaining correlated features.\n",
    "- Balances bias-variance tradeoff effectively.\n",
    "Disadvantages:\n",
    "- Requires tuning two parameters (λ and ρ).\n",
    "- Can be computationally expensive for large datasets.\n",
    "Q4: Common Use Cases for Elastic Net Regression\n",
    "- Genomics: Selecting relevant genes from high-dimensional data.\n",
    "- Finance: Predicting stock prices with correlated indicators.\n",
    "- Marketing: Identifying key factors influencing customer behavior.\n",
    "- Healthcare: Diagnosing diseases using multiple biomarkers.\n",
    "Q5: How to Interpret Coefficients in Elastic Net Regression\n",
    "- Non-zero coefficients: Important features retained.\n",
    "- Zero coefficients: Features eliminated (similar to Lasso).\n",
    "- Shrunken coefficients: Less influential features are penalized but retained.\n",
    "Q6: Handling Missing Values in Elastic Net Regression\n",
    "- Imputation methods:\n",
    "- Mean/Median imputation (simple but may introduce bias).\n",
    "- KNN imputation (uses nearest neighbors for better accuracy).\n",
    "- Multiple imputation (generates multiple plausible values).\n",
    "- Dropping missing values if they are minimal.\n",
    "Q7: Using Elastic Net Regression for Feature Selection\n",
    "Elastic Net shrinks some coefficients to zero, effectively removing irrelevant features. It is useful when:\n",
    "- The dataset has many correlated features.\n",
    "- You need automatic feature selection without manual intervention.\n",
    "Q8: Pickling and Unpickling an Elastic Net Regression Model in Python\n",
    "To save (pickle) a trained model:\n",
    "import pickle\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "with open(\"elastic_net_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "\n",
    "To load (unpickle) the model:\n",
    "with open(\"elastic_net_model.pkl\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "\n",
    "Q9: Purpose of Pickling a Model in Machine Learning\n",
    "Pickling allows you to save a trained model for later use, avoiding the need to retrain it. This is useful for:\n",
    "- Deployment: Using the model in production.\n",
    "- Sharing: Transferring models between systems.\n",
    "- Reproducibility: Ensuring consistent results across runs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aea45e-f12d-466e-8315-155c9e137998",
   "metadata": {},
   "outputs": [],
   "source": [
    "Assignment 6\n",
    "Q1: Key Steps in Building an End-to-End Web Application\n",
    "Building a web application involves several stages:\n",
    "- Planning & Design – Define requirements, wireframe UI, and choose tech stack.\n",
    "- Frontend Development – Build the user interface using HTML, CSS, JavaScript, and frameworks like React or Angular.\n",
    "- Backend Development – Set up the server, database, and APIs using Node.js, Django, or Flask.\n",
    "- Database Management – Choose a database (SQL or NoSQL) and design schema.\n",
    "- Authentication & Security – Implement user authentication, encryption, and security best practices.\n",
    "- Testing & Debugging – Perform unit, integration, and user testing.\n",
    "- Deployment – Host the application on cloud services like AWS, Azure, or Google Cloud.\n",
    "- Monitoring & Maintenance – Use logging, analytics, and updates to ensure smooth operation.\n",
    "Q2: Traditional Web Hosting vs. Cloud Hosting\n",
    "- Traditional Hosting – Websites are hosted on a single physical server, often shared or dedicated.\n",
    "- Cloud Hosting – Websites are hosted on multiple virtual servers, offering scalability and reliability.\n",
    "- Key Differences:\n",
    "- Scalability – Cloud hosting scales dynamically, while traditional hosting has fixed resources.\n",
    "- Cost – Cloud hosting follows a pay-as-you-go model, while traditional hosting has fixed pricing.\n",
    "- Performance – Cloud hosting offers better uptime and load balancing.\n",
    "Q3: Choosing the Right Cloud Provider\n",
    "Factors to consider:\n",
    "- Scalability – Ability to handle traffic spikes.\n",
    "- Security – Compliance with industry standards.\n",
    "- Cost – Pay-as-you-go vs. fixed pricing.\n",
    "- Integration – Compatibility with existing tools.\n",
    "- Support & Reliability – Uptime guarantees and customer service.\n",
    "Q4: Designing a Responsive User Interface\n",
    "Best practices:\n",
    "- Mobile-First Approach – Design for small screens first.\n",
    "- Flexible Layouts – Use CSS Grid and Flexbox.\n",
    "- Optimized Images – Ensure fast loading times.\n",
    "- User-Friendly Navigation – Keep menus simple and accessible.\n",
    "- Testing on Multiple Devices – Ensure compatibility across screen sizes.\n",
    "Q5: Integrating a Machine Learning Model with UI for Algerian Forest Fires Project\n",
    "To integrate a machine learning model:\n",
    "- Train & Save Model – Use Python libraries like TensorFlow or Scikit-learn.\n",
    "- Deploy as an API – Use Flask or FastAPI to expose the model.\n",
    "- Frontend Integration – Send user inputs to the API and display predictions.\n",
    "- Libraries & APIs:\n",
    "- Flask/FastAPI – For API development.\n",
    "- TensorFlow.js – For running ML models in the browser.\n",
    "- REST APIs – For communication between frontend and backend.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b78cbbe-6952-4c3a-80cc-e4856e48098d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff83765-7b7d-4ded-b1d4-2d781251b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic Regression  Assignment 1\n",
    "\n",
    "\n",
    "Q1: Key Features of the Wine Quality Dataset\n",
    "The wine quality dataset includes features such as:\n",
    "- Fixed acidity: Impacts tartness and stability.\n",
    "- Volatile acidity: High levels can lead to an unpleasant vinegar taste.\n",
    "- Citric acid: Adds freshness and enhances flavor.\n",
    "- Residual sugar: Affects sweetness and mouthfeel.\n",
    "- Chlorides: Influences saltiness.\n",
    "- Free sulfur dioxide: Helps prevent oxidation.\n",
    "- Total sulfur dioxide: Preserves wine but excessive amounts can cause off-flavors.\n",
    "- Density: Related to alcohol and sugar content.\n",
    "- pH: Determines acidity balance.\n",
    "- Sulphates: Contributes to antimicrobial properties and enhances aroma.\n",
    "- Alcohol: Affects body and taste.\n",
    "- Quality score: The target variable, rated between 0 and 10.\n",
    "Each feature plays a role in determining the sensory and chemical properties of wine, which collectively influence its quality.\n",
    "Q2: Handling Missing Data in the Wine Quality Dataset\n",
    "Common techniques for handling missing data include:\n",
    "- Mean/Median Imputation: Simple and effective but may not capture variability.\n",
    "- K-Nearest Neighbors (KNN) Imputation: Uses similar data points but can be computationally expensive.\n",
    "- Regression Imputation: Predicts missing values based on other features but assumes linear relationships.\n",
    "- Multiple Imputation: Generates multiple plausible values, improving robustness.\n",
    "Choosing the right method depends on the dataset's characteristics and the impact of missing values on model performance.\n",
    "Q3: Factors Affecting Students' Performance in Exams\n",
    "Key factors include:\n",
    "- Personal factors: Study habits, motivation, and health.\n",
    "- Classroom environment: Teacher quality, peer influence, and resources.\n",
    "- Socioeconomic status: Family support, financial stability, and access to learning materials.\n",
    "- Psychological factors: Stress levels, self-confidence, and test anxiety.\n",
    "Statistical techniques like regression analysis, correlation studies, and hypothesis testing can help identify the most influential factors.\n",
    "Q4: Feature Engineering for Student Performance Dataset\n",
    "Feature engineering involves:\n",
    "- Selecting relevant variables: Attendance, study hours, parental education, and extracurricular activities.\n",
    "- Transforming data: Normalization, binning, and polynomial features.\n",
    "- Handling categorical data: One-hot encoding for gender, socioeconomic status, etc.\n",
    "- Reducing dimensionality: PCA or feature selection techniques.\n",
    "These steps improve model accuracy and interpretability.\n",
    "Q5: Exploratory Data Analysis (EDA) on Wine Quality Dataset\n",
    "EDA involves:\n",
    "- Visualizing distributions: Histograms, box plots, and density plots.\n",
    "- Checking for non-normality: Features like volatile acidity and chlorides often exhibit skewed distributions.\n",
    "- Applying transformations:\n",
    "- Log transformation: Reduces skewness.\n",
    "- Box-Cox transformation: Adjusts non-normal distributions.\n",
    "- Standardization: Ensures consistent scaling.\n",
    "These techniques enhance model performance and interpretability.\n",
    "Q6: Principal Component Analysis (PCA) on Wine Quality Dataset\n",
    "PCA helps reduce dimensionality while retaining variance. To explain 90% of the variance, typically around 5–7 principal components are required, depending on the dataset's structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6440604-57d4-46de-868b-dc1c11e09548",
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic Regression  Assignment 2\n",
    "\n",
    "Q1: Purpose of Grid Search CV in Machine Learning\n",
    "Grid Search CV is used for hyperparameter tuning, helping find the best combination of parameters for a model. It works by:\n",
    "- Defining a grid of possible hyperparameter values.\n",
    "- Training the model on each combination using cross-validation.\n",
    "- Selecting the best-performing set based on evaluation metrics.\n",
    "Q2: Difference Between Grid Search CV and Randomized Search CV\n",
    "- Grid Search CV: Exhaustively tests all possible hyperparameter combinations.\n",
    "- Randomized Search CV: Randomly selects a subset of hyperparameter combinations.\n",
    "- When to choose:\n",
    "- Use Grid Search when the search space is small and computational resources are available.\n",
    "- Use Randomized Search when the search space is large and you need faster results.\n",
    "Q3: What is Data Leakage & Why is it a Problem?\n",
    "Data leakage occurs when information from outside the training dataset is inadvertently used during model training, leading to overly optimistic performance that fails in real-world scenarios. Example: Using future stock prices as a feature when predicting stock trends.\n",
    "Q4: Preventing Data Leakage\n",
    "- Split data properly: Ensure training and test sets are truly independent.\n",
    "- Apply transformations correctly: Perform feature scaling after splitting data.\n",
    "- Avoid target leakage: Ensure features do not contain information about the target variable.\n",
    "Q5: What is a Confusion Matrix?\n",
    "A confusion matrix is a table summarizing a classification model’s performance. It includes:\n",
    "- True Positives (TP): Correctly predicted positive cases.\n",
    "- True Negatives (TN): Correctly predicted negative cases.\n",
    "- False Positives (FP): Incorrectly predicted positive cases.\n",
    "- False Negatives (FN): Incorrectly predicted negative cases.\n",
    "Q6: Difference Between Precision & Recall\n",
    "- Precision: Measures how many predicted positives are actually correct. [ \\text{Precision} = \\frac{TP}{TP + FP} ]\n",
    "- Recall: Measures how many actual positives were correctly predicted. [ \\text{Recall} = \\frac{TP}{TP + FN} ] Trade-off: High precision reduces false positives, while high recall reduces false negatives.\n",
    "Q7: Interpreting a Confusion Matrix for Errors\n",
    "- High FP: Model is overpredicting positives (e.g., falsely classifying spam emails).\n",
    "- High FN: Model is missing positives (e.g., failing to detect fraud cases).\n",
    "- Balanced TP/TN: Model is performing well.\n",
    "Q8: Common Metrics Derived from a Confusion Matrix\n",
    "- Accuracy: ( \\frac{TP + TN}{TP + TN + FP + FN} )\n",
    "- Precision: ( \\frac{TP}{TP + FP} )\n",
    "- Recall: ( \\frac{TP}{TP + FN} )\n",
    "- F1-Score: Harmonic mean of precision and recall.\n",
    "Q9: Relationship Between Accuracy & Confusion Matrix\n",
    "Accuracy is influenced by TP, TN, FP, and FN. However, in imbalanced datasets, accuracy can be misleading. A model predicting the majority class correctly may have high accuracy but poor recall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a028aa-b551-41c1-80f4-e01ff0a429cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
